# Setup for GCP and Terraform

## Local Setup for GCP

_([Video source](https://www.youtube.com/watch?v=Hajwnmj0xfQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=6))_

GCP is organized around _projects_. You may create a project and access all available GCP resources and services from the project dashboard.

We will now create a project and a _service account_, and we will download the authentication keys to our computer. A _service account_ is like a user account but for apps and workloads; you may authorize or limit what resources are available to your apps with service accounts.

>You can jump to the [next section](1_intro.md#gcp-setup-for-access) if you already know how to do this.

Please follow these steps:

1. Create an account on GCP. You should receive $300 in credit when signing up on GCP for the first time with an account.
1. Setup a new project and write down the Project ID.
    1. From the GCP Dashboard, click on the drop down menu next to the _Google Cloud Platform_ title to show the project list and click on _New project_.
    1. Give the project a name. We will use `dtc-de` in this example. You can use the autogenerated Project ID (this ID must be unique to all of GCP, not just your account). Leave the organization as _No organization_. Click on _Create_.
    1. Back on the dashboard, make sure that your project is selected. Click on the previous drop down menu to select it otherwise.
1. Setup a service account for this project and download the JSON authentication key files.
    1. _IAM & Admin_ > _Service accounts_ > _Create service account_
    1. Provide a service account name. We will use `dtc-de-user`. Leave all other fields with the default values. Click on _Create and continue_.
    1. Grant the Viewer role (_Basic_ > _Viewer_) to the service account and click on _Continue_
    1. There is no need to grant users access to this service account at the moment. Click on _Done_.
    1. With the service account created, click on the 3 dots below _Actions_ and select _Manage keys_.
    1. _Add key_ > _Create new key_. Select _JSON_ and click _Create_. The files will be downloaded to your computer. Save them to a folder and write down the path.
1. Download the [GCP SDK](https://cloud.google.com/sdk/docs/quickstart) for local setup. Follow the instructions to install and connect to your account and project.
1. Set the environment variable to point to the auth keys.
    1. The environment variable name is `GOOGLE_APPLICATION_CREDENTIALS`
    1. The value for the variable is the path to the json authentication file you downloaded previously.
    1. Check how to assign environment variables in your system and shell. In bash, the command should be:
        ```bash
        export GOOGLE_APPLICATION_CREDENTIALS="<path/to/authkeys>.json"
        ```
    1. Refresh the token and verify the authentication with the GCP SDK:
        ```bash
        gcloud auth application-default login
        ```

You should now be ready to work with GCP.

## GCP setup for access

_([Video source](https://www.youtube.com/watch?v=Hajwnmj0xfQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=6))_

In the following chapters we will setup a _Data Lake_ on Google Cloud Storage and a _Data Warehouse_ in BigQuery. We will explore these concepts in future lessons but a Data Lake is where we would usually store data and a Data Warehouse provides a more structured way to access this data.

We need to setup access first by assigning the Storage Admin, Storage Object Admin, BigQuery Admin and Viewer IAM roles to the Service Account, and then enable the `iam` and `iamcredentials` APIs for our project.

Please follow these steps:

1. Assign the following IAM Roles to the Service Account: Storage Admin, Storage Object Admin, BigQuery Admin and Viewer.
    1. On the GCP Project dashboard, go to _IAM & Admin_ > _IAM_
    1. Select the previously created Service Account and edit the permissions by clicking on the pencil shaped icon on the left.
    1. Add the following roles and click on _Save_ afterwards:
        * `Storage Admin`: for creating and managing _buckets_.
        * `Storage Object Admin`: for creating and managing _objects_ within the buckets.
        * `BigQuery Admin`: for managing BigQuery resources and data.
        * `Viewer` should already be present as a role.
1. Enable APIs for the project (these are needed so that Terraform can interact with GCP):
   * https://console.cloud.google.com/apis/library/iam.googleapis.com
   * https://console.cloud.google.com/apis/library/iamcredentials.googleapis.com
1. Make sure that the `GOOGLE_APPLICATION_CREDENTIALS` environment variable is set.

## Local Setup for Terraform

### Prerequisites
- Terraform client installation: https://www.terraform.io/downloads
- Cloud Provider account: https://console.cloud.google.com/

### Concepts

#### Introduction
1. What is [Terraform](https://www.terraform.io)?
   * open-source tool by [HashiCorp](https://www.hashicorp.com), used for provisioning infrastructure resources
   * supports DevOps best practices for change management
   * Managing configuration files in source control to maintain an ideal provisioning state 
     for testing and production environments
2. What is IaC?
   * Infrastructure-as-Code
   * build, change, and manage your infrastructure in a safe, consistent, and repeatable way 
     by defining resource configurations that you can version, reuse, and share.
3. Some advantages
   * Infrastructure lifecycle management
   * Version control commits
   * Very useful for stack-based deployments, and with cloud providers such as AWS, GCP, Azure, K8S…
   * State-based approach to track resource changes throughout deployments

#### Files
* `main.tf`
* `variables.tf`
* Optional: `resources.tf`, `output.tf`
* `.tfstate`

#### Declarations
* `terraform`: configure basic Terraform settings to provision your infrastructure
   * `required_version`: minimum Terraform version to apply to your configuration
   * `backend`: stores Terraform's "state" snapshots, to map real-world resources to your configuration.
      * `local`: stores state file locally as `terraform.tfstate`
   * `required_providers`: specifies the providers required by the current module
* `provider`:
   * adds a set of resource types and/or data sources that Terraform can manage
   * The Terraform Registry is the main directory of publicly available providers from most major infrastructure platforms.
* `resource`
  * blocks to define components of your infrastructure
  * Project modules/resources: google_storage_bucket, google_bigquery_dataset, google_bigquery_table
* `variable` & `locals`
  * runtime arguments and constants


### Setup for VM Instances to Access GCP

```
./terraform.sh
```
Add path to google default credential
```
export GOOGLE_APPLICATION_CREDENTIALS="path/to/your/service-account"
```
You will need to give extra permissions for VM instances, dataproc and firewall in IAM roles in service account page in GCP.
- For VM instances, grant the `Compute Admin` role. 
- For Dataproc clusters, grant the `Dataproc Editor` or `Dataproc Admin` role. 
- For firewall rules, grant the `Compute Network Admin` or `Network Admin` role. 


### Execution (Ubuntu setup)

```shell
# Initialize state file (.tfstate): initializes & configures the backend, 
# installs plugins/providers, & checks out an existing configuration from a version control.
# For example, google provider in our case and all the existing configuration 
terraform init

# Matches/previews local changes against a remote state, and proposes an Execution Plan.
terraform plan

# The changes that are detected from the terraform plan, any resources to be deleted 
# or new resources to be created or any existing resources need to be updated in configuration. 
# (Asks for approval to the proposed plan, and applies changes to cloud)
# For example, increasing the memory size of a particular cluster.
terraform apply

# Delete infra after your work, to avoid costs on any running services
terraform destroy
```

### Generate SSH access
If you connect to VMs using the Google Cloud console or the Google Cloud CLI, Compute Engine creates SSH keys on your behalf. For more information on how Compute Engine configures and stores keys, see [About SSH connections](https://cloud.google.com/compute/docs/instances/ssh).

If you connect to VMs using third party tools or OpenSSH, you need to add a key to your VM before you can connect. If you don't have an SSH key, you must create one. VMs accept the key formats listed in the sshd_config file.

Follow the link instruction: [https://cloud.google.com/compute/docs/connect/create-ssh-keys
](https://cloud.google.com/compute/docs/connect/create-ssh-keys)

On Linux and macOS workstations, use the `ssh-keygen` utility to create a new SSH key pair. The following example creates an RSA key pair.

```
ssh-keygen -t rsa -f ~/.ssh/KEY_FILENAME -C USERNAME -b 2048
ssh-keygen -t rsa -f ~/.ssh/gcp -C de_zc_2023 -b 2048

Output:
Generating public/private rsa key pair.
…
The key's randomart image is:
+---[RSA 2048]----+
|       ...       |
+----[SHA256]-----+

```

Now we created two keys, one is a `private key` and the other key is a `public key`. You don’t need to show the private key to anyone, and the public key can be shared.

```
ls
cp      gcp.pub     id_ed25519      id_ed25519.pub      known_hosts

cat gcp.pub 
ssh-rsa AAAA ... e5m1RW3/43j de_zc_2023
```

Next, we put this public key (ssh-rsa AAAA ... e5m1RW3/43j de_zc_2023) to google cloud. In Compute engine >> Select Metadata >> SSH key tab >> ADD SSH KEY >> put public key >> Save to finish

```
TODO: input gcp_ssh_key_setup.png from images folder for visualize result
```

### Connect to VM instances
Connect to Linux virtual machine (VM) instances that have external IP addresses.
```
ssh -i PATH_TO_PRIVATE_KEY USERNAME@EXTERNAL_IP
ssh -i ~/.ssh/gcp de_zc_2023@35.201.188.31

Output: 
de_zc_2023@kafka-instance:~$
```

### References
- https://learn.hashicorp.com/collections/terraform/gcp-get-started
- https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/1_terraform_overview.md